import numpy as np
import pandas as pd
from src import config

# ==========================================
# 1. CONFIGURATION
# ==========================================

# Paths
TEST_FILE_PATH = config.PROCESSED_DATA_PATH / 'movielens-32m_global_temporal_split/test.csv'
PREDICTIONS_FILE_PATH = config.TEMP_DATA_PATH / "reduced_candidates_non_zero_recall_filtered.csv"  # The CSV generated by your previous script

# Evaluation Settings
K_METRICS = [10, 20]  # The cutoffs you want to evaluate


# ==========================================
# 2. HELPER FUNCTIONS
# ==========================================

def calculate_metrics_at_k(hits, k, num_ground_truth):
    """
    Calculates Recall, NDCG, and MRR for a specific cutoff K.

    :param hits: List of binary hits (1 if item in ground truth, 0 otherwise) sorted by rank
    :param k: The cutoff rank
    :param num_ground_truth: Total number of positive items for this user
    """
    # 1. Truncate to K
    hits_k = hits[:k]

    if num_ground_truth == 0:
        return 0.0, 0.0, 0.0

    # 2. Recall @ K
    recall = sum(hits_k) / num_ground_truth

    # 3. NDCG @ K
    dcg = sum([hit / np.log2(idx + 2) for idx, hit in enumerate(hits_k)])
    # IDCG (Ideal DCG) is if the top 'num_ground_truth' items were all hits
    idcg_count = min(num_ground_truth, k)
    idcg = sum([1 / np.log2(idx + 2) for idx in range(idcg_count)])

    ndcg = dcg / idcg if idcg > 0 else 0.0

    # 4. MRR @ K
    mrr = 0.0
    for idx, hit in enumerate(hits_k):
        if hit:
            mrr = 1.0 / (idx + 1)
            break

    return recall, ndcg, mrr


# ==========================================
# 3. LOAD DATA
# ==========================================

print("Loading data...")

# 1. Load Predictions First (To get the list of relevant users)
# Assuming columns: [user_id, item_id, rank, score]
preds_df = pd.read_csv(PREDICTIONS_FILE_PATH)

# Ensure predictions are sorted by User and Rank (ascending)
preds_df.sort_values(by=['user_id', 'rank'], ascending=[True, True], inplace=True)

# Get the set of users we actually have predictions for
predicted_users_set = set(preds_df['user_id'].unique())
print(f"Loaded predictions for {len(predicted_users_set)} users.")

# 2. Load Ground Truth (Test Set)
# Assuming columns: [user_id, item_id, rating, timestamp]
test_df = pd.read_csv(TEST_FILE_PATH, names=['user_id', 'item_id', 'rating', 'timestamp'])

# --- OPTIMIZATION: Filter Ground Truth Immediately ---
# Only keep ground truth records for users that exist in our predictions
test_df_filtered = test_df[test_df['user_id'].isin(predicted_users_set)].copy()

print(f"Filtered ground truth from {len(test_df)} to {len(test_df_filtered)} records matching the predicted users.")

# 3. Create Dictionary from Filtered Data
# {user_id: {item_id1, item_id2, ...}}
ground_truth_dict = test_df_filtered.groupby('user_id')['item_id'].apply(set).to_dict()

# 4. Group predictions
preds_grouped = preds_df.groupby('user_id')['item_id'].apply(list)

# ==========================================
# 4. EVALUATION LOOP
# ==========================================

print(f"Evaluating {len(preds_grouped)} users against {len(ground_truth_dict)} ground truth records...")

# Initialize storage for results: {10: {'Recall': [], ...}, 20: {'Recall': [], ...}}
results = {k: {'Recall': [], 'NDCG': [], 'MRR': []} for k in K_METRICS}

# Iterate only over users who exist in both predictions and ground truth
common_users = set(preds_grouped.index).intersection(set(ground_truth_dict.keys()))

for user_id in common_users:
    ground_truth_items = ground_truth_dict[user_id]
    ranked_items = preds_grouped[user_id]

    # Pre-calculate hits list (1 if item is in ground truth, 0 otherwise)
    # This assumes 'ranked_items' is already sorted by rank (1, 2, 3...)
    hits = [1 if item in ground_truth_items else 0 for item in ranked_items]
    num_gt = len(ground_truth_items)

    # Compute metrics for each K
    for k in K_METRICS:
        r, n, m = calculate_metrics_at_k(hits, k, num_gt)
        results[k]['Recall'].append(r)
        results[k]['NDCG'].append(n)
        results[k]['MRR'].append(m)

# ==========================================
# 5. REPORTING
# ==========================================

print("\n" + "=" * 40)
print("       EVALUATION REPORT       ")
print("=" * 40)

for k in K_METRICS:
    print(f"\n--- Metrics @ K={k} ---")

    avg_recall = np.mean(results[k]['Recall'])
    avg_ndcg = np.mean(results[k]['NDCG'])
    avg_mrr = np.mean(results[k]['MRR'])

    print(f"Recall@{k}: {avg_recall:.4f}")
    print(f"NDCG@{k}:   {avg_ndcg:.4f}")
    print(f"MRR@{k}:    {avg_mrr:.4f}")

print("\n" + "=" * 40)